{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to build Language Detection model using the euclidian distances between n-gram frequency vectors. \n",
    "\n",
    "We will need to choose the n value. Here are a few hypotheses:\n",
    "\n",
    "* 1-grams are just individual charachters and will not retain any sequential information about the data, which seems  important for language detection.\n",
    "\n",
    "* 2-grams are pairs of chachters. 2 could possibily be an adequate n value, however, 2-grams only capture immediate sequencial information.\n",
    "\n",
    "* 3-grams or 4-grams intuitively seem to be enough to capture most of the useful sequential information.\n",
    "\n",
    "* 5-grams and larger may contain many whole words, in effect becoming more like a \"dictionary\" which will be ineffective when there are words in the test set that are not part of the training set. We can think of this as being analogous to overfitting.\n",
    "\n",
    "Let's test these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of traning data to be read in bytes\n",
    "TRAINING_DATA_SIZE = 2000000\n",
    "\n",
    "LANGUAGES = [\n",
    "    'sv', 'da', 'de', 'nl', 'en', 'fr', 'es', 'pt', 'it', 'ro', 'et',\n",
    "    'fi','lt', 'lv', 'pl', 'sk', 'cs', 'sl', 'hu', 'bg',  'el'\n",
    "]\n",
    "\n",
    "# Files names\n",
    "files = [\n",
    "    \"train/europarl-v7.{lang}-en.{lang}\".format(lang=x)\n",
    "    for x in LANGUAGES\n",
    "]\n",
    "\n",
    "# Open files\n",
    "corpus_raw = [\n",
    "    open(x).read(TRAINING_DATA_SIZE)\n",
    "    for x in files\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove punctuations. While these probably not entirely useless for language detection, these will increase the number of n-grams which would reduce our confidence in the n-gram frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    re.sub(r'[?‚Äù_\"%()!--+,:;./\\]\\[\\xad\\n0-9\\=<>]', '', x)\n",
    "    for x in corpus_raw\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the n-grams from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-values of [1, 2, 3, 4, 5, 6] produce [186, 5957, 61537, 300332, 819272, 1417028] n-grams repectively\n",
      "Some 3-grams look like this:    (0, 48831)\t1\n",
      "  (0, 110313)\t1\n",
      "  (0, 112085)\t1\n",
      "  (0, 191963)\t1\n",
      "  (0, 119752)\t1\n",
      "  (0, 125742)\t1\n",
      "  (0, 121997)\t1\n",
      "  (0, 151276)\t1\n",
      "  (0, 44845)\t1\n",
      "  (0, 66126)\t1\n",
      "  (0, 197099)\t1\n",
      "  (0, 44817)\t1\n",
      "  (0, 82393)\t1\n",
      "  (0, 102107)\t1\n",
      "  (0, 16801)\t1\n",
      "  (0, 180640)\t1\n",
      "  (0, 124218)\t1\n",
      "  (0, 61091)\t1\n",
      "  (0, 61491)\t1\n",
      "  (0, 54769)\t1\n",
      "  (0, 39844)\t1\n",
      "  (0, 64217)\t1\n",
      "  (0, 178777)\t1\n",
      "  (0, 32269)\t1\n",
      "  (0, 16206)\t1\n",
      "  :\t:\n",
      "  (4, 53765)\t261\n",
      "  (4, 110539)\t5782\n",
      "  (4, 21094)\t1655\n",
      "  (4, 99210)\t15\n",
      "  (4, 23510)\t1\n",
      "  (4, 139281)\t1038\n",
      "  (4, 22735)\t29\n",
      "  (4, 139253)\t3\n",
      "  (4, 151330)\t14\n",
      "  (4, 184085)\t2390\n",
      "  (4, 57982)\t2448\n",
      "  (4, 2200)\t2480\n",
      "  (4, 99337)\t25\n",
      "  (4, 133004)\t32\n",
      "  (4, 81271)\t599\n",
      "  (4, 159998)\t3489\n",
      "  (4, 163016)\t2539\n",
      "  (4, 56479)\t318\n",
      "  (4, 159047)\t136\n",
      "  (4, 7204)\t34\n",
      "  (4, 117849)\t6\n",
      "  (4, 21697)\t62\n",
      "  (4, 63798)\t16\n",
      "  (4, 17896)\t14\n",
      "  (4, 55616)\t2\n"
     ]
    }
   ],
   "source": [
    "#N_VALUES = [1,2,3,4,5,6,7]\n",
    "N_VALUES = [1,2,3,4,5,6]\n",
    "\n",
    "count_vectorizers = [\n",
    "    CountVectorizer(ngram_range=(n, n), analyzer='char_wb')\n",
    "    for n in N_VALUES\n",
    "]\n",
    "\n",
    "counts_array = [\n",
    "    count_vectorizer.fit_transform(corpus)\n",
    "    for count_vectorizer in count_vectorizers\n",
    "]\n",
    "\n",
    "print(\"n-values of {} produce {} n-grams repectively\".format(\n",
    "    N_VALUES, list(map(lambda x: x.shape[1], counts_array)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Term Weights\n",
    "$$Term\\ Weight = log(Term\\ frequency\\ in\\ a\\ particular\\ languge\\ + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_weights(counts):\n",
    "    # Every language should have the same mean count frequency\n",
    "    counts = counts/counts.mean(axis=1)\n",
    "        \n",
    "    # Sublinear transform to ensure no terms have overwhelmingly large weight\n",
    "    counts = np.log(counts+1)\n",
    "\n",
    "    return counts\n",
    "\n",
    "language_weights = [compute_weights(c) for c in counts_array]\n",
    "analyzers = [count_vectorizer.build_analyzer() for count_vectorizer in count_vectorizers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socring test samples\n",
    " We can think of test samples as points in space in the same number of dimentions as our training data. We will treat test samples exactly the same way as our training set. Our prediction will simply be the language with the lowest Euclidean distance from the test sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runing test for n = 1\n",
      "Final er: 5.371428571428571%\n",
      "total right: 19872 \n",
      "total wrong: 1128\n",
      "\n",
      "Runing test for n = 2\n",
      "Final er: 7.719047619047619%\n",
      "total right: 19379 \n",
      "total wrong: 1621\n",
      "\n",
      "Runing test for n = 3\n",
      "Final er: 43.31428571428572%\n",
      "total right: 11904 \n",
      "total wrong: 9096\n",
      "\n",
      "Runing test for n = 4\n",
      "Final er: 66.94285714285714%\n",
      "total right: 6942 \n",
      "total wrong: 14058\n",
      "\n",
      "Runing test for n = 5\n"
     ]
    }
   ],
   "source": [
    "def predict(text, n):\n",
    "    term_weight = compute_weights(count_vectorizers[n-1].transform([text]))    \n",
    "    distances = [np.linalg.norm(term_weight-language_weight) for language_weight in language_weights[n-1]]\n",
    "    return LANGUAGES[np.argmin(distances)]\n",
    "\n",
    "def run_tests(n):\n",
    "    print(\"\\nRuning test for n = {}\".format(n))\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    tests = open('europarl.test')\n",
    "    for x in range(TRAINING_DATA_SIZE):\n",
    "        line = tests.readline()\n",
    "        if line == '':\n",
    "            print(\"Error rate: {}%\".format(100*wrong/(right+wrong)))\n",
    "            return\n",
    "        [lang, text] = line.split('\\t')\n",
    "        if predict(text, n) == lang:\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "\n",
    "for n in N_VALUES:\n",
    "    run_tests(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 1 gives us the best error rate. This is odd since N = 1 takes into account only charchter frequencies. Let look at what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizers[0].transform([\"This is an english sentence\"]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that test samples are very small and will comatin mostly zeros. This is a very low signal to noise ratio. The distances between zero and the actual language values for these dimentions do not provide much information useful for classification. Let's see what would happen if we can consider another space consisting only of dimentions that exist in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(text, n):\n",
    "    term_weight = compute_weights(count_vectorizers[n-1].transform([text]))\n",
    "    \n",
    "    # Inculde only dimentions where term wieght is not zero\n",
    "    relevent_dimentions = [\n",
    "        language_weight[term_weights != 0] for language_weight in language_weights[n-1]\n",
    "    ]\n",
    "    term_weight = term_weights[term_weight != 0]\n",
    "    \n",
    "    distances = [np.linalg.norm(term_weights-c) for c in relevent_dimentions]\n",
    "    return LANGUAGES[np.argmin(distances)]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
