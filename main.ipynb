{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to build Language Detection model that tries to minimize the euclidian distances between n-gram frequency vectors. \n",
    "\n",
    "We will need to choose the n value. Here are a few hypotheses:\n",
    "\n",
    "* 1-grams are just individual charachters and will not retain any sequential information about the data, which seems  important for language detection.\n",
    "\n",
    "* 2-grams are pairs of chachters. 2 could possibily be an adequate n value, however, 2-grams only capture immediate sequencial information.\n",
    "\n",
    "* 3-grams or 4-grams intuitively seem to be enough to capture most of the useful sequential information.\n",
    "\n",
    "* 5-grams and larger may overfit to the training data. These will contain many whole words, becoming more like a \"dictionary\" which will be ineffective when there are words in the test set that are not part of the training set.\n",
    "\n",
    "Let's test these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Size of traning data to be read in bytes\n",
    "TRAINING_DATA_SIZE = 2000000\n",
    "\n",
    "LANGUAGES = [\n",
    "    'sv', 'da', 'de', 'nl', 'en', 'fr', 'es', 'pt', 'it', 'ro', 'et',\n",
    "    'fi','lt', 'lv', 'pl', 'sk', 'cs', 'sl', 'hu', 'bg',  'el'\n",
    "]\n",
    "\n",
    "# Files names\n",
    "files = [\n",
    "    \"train/europarl-v7.{lang}-en.{lang}\".format(lang=x)\n",
    "    for x in LANGUAGES\n",
    "]\n",
    "\n",
    "# Open files\n",
    "corpus_raw = [\n",
    "    open(x).read(TRAINING_DATA_SIZE)\n",
    "    for x in files\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove punctuations. While these probably not entirely useless for language detection, these will increase the number of n-grams which would reduce our confidence in the n-gram frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    re.sub(r'[?‚Äù_\"%()!--+,:;./\\]\\[\\xad\\n0-9\\=<>]', '', x)\n",
    "    for x in corpus_raw\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the n-grams from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-values of [1, 2, 3, 4, 5, 6, 7] produce [186, 5957, 61537, 300332, 819272, 1417028, 1834018] n-grams repectively\n"
     ]
    }
   ],
   "source": [
    "N_VALUES = [1,2,3,4,5,6,7]\n",
    "\n",
    "count_vectorizers = [\n",
    "    CountVectorizer(ngram_range=(n, n), analyzer='char_wb')\n",
    "    for n in N_VALUES\n",
    "]\n",
    "\n",
    "counts_array = [\n",
    "    count_vectorizer.fit_transform(corpus)\n",
    "    for count_vectorizer in count_vectorizers\n",
    "]\n",
    "\n",
    "print(\"n-values of {} produce {} n-grams repectively\".format(\n",
    "    N_VALUES, list(map(lambda x: x.shape[1], counts_array)\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Term Weights\n",
    "$$Term\\ Weight = log(Term\\ frequency\\ in\\ a\\ particular\\ languge\\ + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_weights(counts):\n",
    "    # Every language should have the same mean count frequency\n",
    "    counts = counts/counts.mean(axis=1)\n",
    "        \n",
    "    # Sublinear transform to ensure no terms have overwhelmingly large weight\n",
    "    # + 1 to ensure non negative values\n",
    "    counts = np.log(counts+1)\n",
    "\n",
    "    return counts\n",
    "\n",
    "language_weights = [compute_weights(c) for c in counts_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socring test samples\n",
    "We will treat test samples exactly the same way as our training data. Our prediction will simply be the language with the lowest Euclidean distance from the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Runing test for n = 1\n",
      "Error rate: 5.371428571428571%\n",
      "\n",
      "Runing test for n = 2\n",
      "Error rate: 7.719047619047619%\n",
      "\n",
      "Runing test for n = 3\n",
      "Error rate: 43.31428571428572%\n",
      "\n",
      "Runing test for n = 4\n",
      "Error rate: 66.94285714285714%\n",
      "\n",
      "Runing test for n = 5\n",
      "Error rate: 93.72857142857143%\n",
      "\n",
      "Runing test for n = 6\n",
      "Error rate: 95.23809523809524%\n",
      "\n",
      "Runing test for n = 7\n"
     ]
    }
   ],
   "source": [
    "def predict(text, n):\n",
    "    term_weight = compute_weights(count_vectorizers[n-1].transform([text]))    \n",
    "    distances = [\n",
    "        np.linalg.norm(term_weight-language_weight) for language_weight in language_weights[n-1]\n",
    "    ]\n",
    "    return LANGUAGES[np.argmin(distances)]\n",
    "\n",
    "def run_tests(n):\n",
    "    print(\"\\nRuning test for n = {}\".format(n))\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    tests = open('europarl.test')\n",
    "    for x in range(TRAINING_DATA_SIZE):\n",
    "        line = tests.readline()\n",
    "        if line == '':\n",
    "            print(\"Error rate: {}%\".format(100*wrong/(right+wrong)))\n",
    "            return\n",
    "        [lang, text] = line.split('\\t')\n",
    "        if predict(text, n) == lang:\n",
    "            right = right + 1\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "\n",
    "for n in N_VALUES:\n",
    "    run_tests(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = 1 gives us the best error rate. This is odd since N = 1 takes into account only charchter frequencies. Let look at what is going on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizers[0].transform([\"This is an english sentence\"]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that test samples are very small and will comatin mostly zeros. This is a very low signal to noise ratio. The distances between zero and the actual language values for these dimentions do not provide much information useful for classification. Let's see what would happen if we can consider another space consisting only of dimentions that exist in the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, n):\n",
    "    term_weight = compute_weights(count_vectorizers[n-1].transform([text]))\n",
    "    \n",
    "    # Inculde only dimentions where term wieght is not zero\n",
    "    non_zero_dimentiuons = term_weight != 0\n",
    "    relevent_dimentions = [\n",
    "        language_weight[non_zero_dimentiuons] for language_weight in language_weights[n-1]\n",
    "    ]\n",
    "    term_weight = term_weight[non_zero_dimentiuons]\n",
    "    \n",
    "    distances = [np.linalg.norm(term_weight-language_weight) for language_weight in relevent_dimentions]\n",
    "    return LANGUAGES[np.argmin(distances)]\n",
    "\n",
    "for n in N_VALUES:\n",
    "    run_tests(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
